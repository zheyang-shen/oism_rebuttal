## Reviewer VsSe

We thank the reviewer for your effort and insights, which will contribute to our effort to improve our current manuscript. We list our response to your concerns below. 

>**Novelty.** According to the authors, “Markov DMs remain the default choice for forward processes,” and I agree with this statement. Broadly speaking, diffusion and score matching models already rely on Markovian assumptions heavily, and theoretical properties in Sections 2 and 3 are also applicable to contemporary models. From my understanding, Bakry's Makrov semigroup (or Bakry-Émery theory in general) was developed to study analytic properties of nontrivial probabilistic structures of various geometries. From this aspect, I believe the paper should have more focus on the theory's direct implication such as solving non-Euclidean Riemannian manifolds or infinite-dimensional functional space. The theoretical part contains many known math results without a new theorem. Although the theory part is written concrete and general, the derivations are mostly known from a mathematical standpoint, the assumption does not predict new analytic outcomes of diffusion algorithms. Therefore, the originality of this work is mainly in Section 2.2 Eqs. (8-10) and Section (10). Therefore, the significance of discussion in the context of machine learning should be validated with new findings and actual experiments.

We thank the reviewer for this comment, and explain our response below. 

From our point of view, diffusion models do _not_ rely heavily on the Markov assumption of the forward process, so much so that the term "diffusion model" often becomes a misnomer. If there exists a discrete- or continuously-indexed sequence of distributions that reduces the data distribution to some easy-to-sample "stationary" distribution, we can, in theory, invert that process for generative modelling. Markov diffusion processes remain the most straightforward candidate for such "sequence of distributions", but they are not the only option. The score matching objective also does not rely on any Markovian assumption. Implicit score matching (ISM) can be used to learn the score of any distribution using samples, and denoising score matching (DSM) requires a pair of clean and perturbed distributions, but the type of perturbation does not need to adhere to that of a (Markov) diffusion process. In fact, Bansal et al. [1] illustrate that "diffusion models" are often so general that they can be built on the basis of inverting arbitrary image transforms. Our framework relies on the observation that, when the noising process is Markov, the score functions in different noising levels exhibit spatial and temporal dependencies as prescribed by the underlying noising process, and analyses such dependencies under the framework of Markov diffusion operators. 

It is likely the case that other contemporary papers have noted that, when viewed through the lens of the conditional expectations (Markov semigroup), the expected values of functions evolve deterministically in the forward process; or that the evolution of densities is inherently linked to the underlying forward process. We discuss these related works, as well as their connection to Markov operator theory in Appendix D. However, apart from using the deterministic of first- and second-order moments to produce a Gaussian score function naively [2], it remains unclear how the deterministic nature of conditional expectations link to estimation of the scores, an indispensable component for sample generation. We believe that our findings in the approximation of scores are a substantial novel result in the area of score matching.  

The Bakry-Émery theory indeed mainly focuses on the study of diffusion processes in less straightforward settings, and we think that the theory remains under-explored in the current study of generative modelling. Our work focuses on the most straightforward setting of the Bakry-Émery theory, but the spectral properties of common forward processes are similarly under-explored, and we illustrate in our paper that significant insights can be gathered by treating vanilla score matching in an operator-aware context. The extension of our work to, e.g., a Riemannian manifold or functional setting is left for future work. 

The reviewer is indeed right in pointing out that Sections 1 and 2.1 revise known material, whilst the main methodological novelty is presented in Sections 2.2 and 3. The authors would welcome the suggestion to restructure the paper, moving some of the material from Sections 1 and 2.1 to the appendix, presenting the novel formulation of the implicit score matching loss and the OISM score as a theorem, whose proof would also be moved to the appendix. The authors have expanded the simulation results by providing quantitative metrics for the 2D examples, adding a study of the performance of OISM on the ImageNet dataset, and computing standard performance metrics in the high-dimensional case. We hope to address in future work the question of how the OISM score estimator relates to the ground truth score function and the analytical study of error bounds. 

>**Scope.** The paper's top-down, theory-first approach results in a method whose practical benefits are not clearly articulated. While the proposed score-matching objective in Eq. (10) is a mathematically interesting finding, its advantages for practitioners are unclear. For example, solid description on computational efficiency, and distinct benefits of using various assumptions, such as using forward processes and Hermite polynomials is required. I believe the topic itself is very important in the machine learning community; yet, the paper struggles to find its contributions other than the fact that it is simple enough to be implemented. Therefore, I say the significance is limited until the proposed method is backed by more real-world problems.

We agree that the question of how to leverage the OISM score function in a practical setting remains open, and that we should keep investigating the practical implications of our approach. We appreciate your keen insight regarding Hermite polynomials, namely in that they were chosen out of convenience, as OU process is the sole canonical option for Markov forward processes with a countable spectrum. The practical use of Hermite polynomial for OISM can be somewhat hindered by numerical instability, as expressing the product of 2 Hermite polynomials as a sum requires combinatorial numbers that can become numerically cumbersome.

_ZS:_ I have given the reviewer a concession on the topic of Hermite polynomial as a tool of convenience. Is it ok?

Here we would like to highlight a few instances that can be of practical significance. Even though what we illustrate below might not be directly applicable to practitioners, they remain valuable insights produced by the theoretical basis of our paper. 

_ZS_: I think it would be good appearance-wise to come up with a few points, but I find it quite hard to pinpoint what the reviewer might be looking for. I think the insight about choosing forward process is helpful. 
- **The importance of selecting forward process based on their spectral properties**. We have considered three types of Markov noising processes in the paper: Ornstein-Uhlenbeck process (VP), Brownian motion (VE), and truncated Brownian motion. These processes differ greatly in their spectral properties, with the first and the last having countable spectra, the second having an uncountable spectrum. Therefore, the first and the last are more amenable to the search for informative eigenfunctions under the OISM framework. Moreover, when compared to the Hermite polynomials given by OU, the trigonometric eigenfunctions of the truncated BM enjoy better stability thanks to their boundedness and straightforward interaction, so much so that trigonometric features are able to single-handedly recover low-dimensional target distributions. However, despite its simplicity, truncated BM remains a new concept as a forward process, selected by its appealing spectral property. 
- **The interplay between classical statistical methods**

_ZS_: If I would want to make a 2nd point on practical implications, it might be something about "classical statistical tools such as James-Stein estimator and kernel smoothing are still relevant in studying generative models", but it seems quite pedantic. We can say something on the topic of how rudimentary score estimates, no matter how simple, are still useful in accelerating the training of DMs. 

> **Experiment.** Compared to the theory part, the experimental part is very weak and requires substantial improvements on breath and depth. I strongly recommend adding strong benchmarks and score matching methods such as VP and VE SDEs [Song et al., 2020b], and other DDPMs. I also suggest testing higher dimensional images to see if the proposed operator-informed approach does not suffer from dimensionality.

The authors have justified in the answer above the choices of the forward processes for the dataset originally presented in the paper. We have, since then, done a more quantitative study of the 2D dataset and added simulations and performance metrics for the ImageNet dataset. We agree with the reviewer's point that demonstrating improved performance via OISM compared to DDPM on even higher-dimensional data (e.g. 512 x 512 images) would provide convincing evidence of the benefits of our approach. However, it seems a high expectation for a method to perform consistently well on data of increased complexity, without incorporating additional methods that would act as confounders when assessing performance. Of course, if assessing empirical OISM performance (and its possible limitations) when scaling the data dimension was a binding condition, we would run such experiments. 

> Is there any benefits for learning with forward processes? Can operator-informed approach implemented with backward process?

We do not understand what "learning with forward processes" entails specifically -- if the purpose is to find a forward process beyond OU and BM considered by the paper, but still with a tractable spectrum, then there will be a clear practical benefit in choosing a forward process suitable to the data distribution at hand. A linear time-invariant SDE has a tractable spectrum (Appendix A) as a forward process, and it is possible to vary the parameter of the forward process while maintaining tractability. 

We don't understand what "operator-informed approach in backward process" means -- would you like to clarify?

> Can OISM scalable to generate large 512x512 images?

 Please see our reply above, regarding scalability to higher-dimensional datasets. 

[1] Bansal A, Borgnia E, Chu HM, Li JS, Kazemi H, Huang F, et al. Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise. In: Thirty-seventh Conference on Neural Information Processing Systems. 2023.

[2] Wang, Binxu, and John Vastola. 2024. “The Unreasonable Effectiveness of Gaussian Score Approximation for Diffusion Models and Its Applications.” _Transactions on Machine Learning Research_, July 8.
