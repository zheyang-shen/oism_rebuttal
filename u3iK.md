## Reviewer u3iK: 

>The proposed score matching algorithm appears to heavily rely on the properties of BM and OU processes, whose eigenfunctions are analytically tractable. One possible concern is that optimizing the generalized implicit score matching loss in Eq. (10) may not lead to tractable and effective algorithms when applied to other Markov processes.

Thank you for this comment, and we address the concern from 2 standpoints. 
 - **We often have the freedom to choose which forward process in diffusion modeling.** We focus on BM and OU processes due to their tractability, and we do not consider it as a major weakness as we often are free to choose the type of forward process in the context of diffusion modeling, and these two processes cover most use cases. Moreover, as there is the freedom of choice for the forward process in diffusion modeling, our work opens the possibility to developing novel forward processes with suitable Markov properties. For example, we propose the truncated Brownian motion (Appendix B.2) in order to make use of its countable spectrum, which differs from the standard Brownian motion. It is possible that we can _construct_ forward processes by choosing desirable spectral properties in a data-adaptive manner, and we believe that it is a new point of view that can only be made possible by our viewpoint.
 - **Our operator-centric viewpoint on score matching is still valuable even when the forward process is not fully characterized in its spectrum.** In fact, one can argue that existing work on score-based generative modeling already makes use of implicit score matching (ISM). For example, the original score matching objective by HyvÃ¤rinen was used in the first score-based generative modeling paper. $\mathbb{E}_{\rho_t} \left\Vert \mathbf{s}_t - \nabla \log \rho_t\right\Vert^2 = C + \mathbb{E}_{\rho_t} \left[\left\Vert \mathbf{s}_t\right\Vert^2 + 2\langle \nabla, \mathbf{s}_t\rangle\right],$which is formulated as a special case of the ISM objective in Eq. (9). Instead of formulating $\mathbf{s}$ using eigenfunctions, it was parametrized with a neural network and auto-differentiation was used to handle the calculation of $\langle \nabla, \mathbf{s}\rangle$. While ISM is more computationally expensive compared to the default use of denoising score matching (DSM), it remains an open question whether diffusion models can be better trained using ISM as its objective function. Moreover, we argue that our operator-centric form of ISM in Eq. (10) is more general. It is not difficult to see that while standard diffusion modeling literature does not make use of the semigroup operator $P_t$ to describe the score matching objective, it does so implicitly. For example, $\mathbb{E}_{\rho_t} \left[\left\Vert \mathbf{s}_t\right\Vert^2 + 2\langle \nabla, \mathbf{s}_t\rangle\right] = \mathbb{E}_{\rho_0}P_t\left(\left\Vert \mathbf{s}_t\right\Vert^2 + 2\langle \nabla, \mathbf{s}_t\rangle\right).$ We can see that while the left- and right-hand side of the above equation is equivalent, the l.h.s. suggests that this quantity can _only_ be evaluated by taking samples from the perturbed distribution $\rho_t$, and r.h.s. can be evaluated as long as $P_t$ is computable, which does not necessarily rely on Monte Carlo samples from $\rho_t$. 
It is from the above 2 perspectives that we think our formulation simultaneously informs the most popular choices for diffusion modeling, and our viewpoint of considering Markov operators has benefits that transcends the scope of OU and BM processes. 

>If my understanding is correct, the number of eigenfunctions exponentially increases when the proposed algorithm is applied to high-dimensional data, which could present a significant bottleneck. This would make solving the optimization problem in Eq. (11) computationally expensive and hinder the use of higher-order approximations.

Your insight about the exponentially increasing number of eigenfunctions is correct, and it is indeed a bottleneck if we chose to only utilize eigenfunctions to characterize an intractable distribution. We intend to convey 2 messages via our experiments. (i) When eigenfunctions can be properly enumerated, they alone can be used for sample generation, which provides a training-free alternative to standard diffusion models; (ii) When there is no appropriate way to enumerate eigenfunctions, the capacity of OISM is diminished, but they can still be applied to provide a "good guess" alongside a neural network score estimator. We think that our work serves as a good starting point that can elicit future work in score matching informed by the underlying forward process. 

A notable direction for future work would be to alleviate the burden to enumerate eigenfunctions by choosing them in a data-adaptive manner. We think this is outside the scope of this paper due to our primary focus on solving OISM with the standard enumeration of eigenfunctions. 

>Considering the above weaknesses 1 and 2, the benefit of reformulating the ISM loss in Eq. (9) might remain unclear. Providing more detailed future directions and potentials of the proposed viewpoint would further enhance the significance of the paper.

Thank you for this suggestion, and we agree that we should outline promising future directions of our approach. 

>While the paper presents quantitative results on 2D datasets, a quantitative comparison with DDPM on 2D datasets might be lacking. Including further quantitative results on 2D datasets would better highlight the effectiveness of the proposed method.

It is not yet straightforward to compare with DDPM on 2D dataset, as an OISM-only approach requires no training. 

Comment: with our new insights onto DSM with truncated BM, we can technically compare with DDPM, but the question remains as DDPM requires extended training but OISM does not. 